import asyncio
import logging
import requests
from typing import List, Dict, Any, Optional
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.models import PointStruct
import uuid
from pydantic import BaseModel
import os

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EmbeddingRequest(BaseModel):
    text: str
    model: Optional[str] = os.getenv("EMBEDDING_MODEL", "text-embedding-ada-002")

class EmbeddingResponse(BaseModel):
    embeddings: List[float]

class QdrantService:
    def __init__(self):
        # Get configuration from environment variables
        self.host = os.getenv("QDRANT_HOST", "localhost")
        self.port = int(os.getenv("QDRANT_PORT", "6333"))
        self.api_key = os.getenv("QDRANT_API_KEY")
        self.collection_name = os.getenv("QDRANT_COLLECTION_NAME", "book_embeddings")

        # Initialize Qdrant client and test connection
        self.client = None
        self.connected = False

        try:
            if self.api_key:
                self.client = QdrantClient(
                    url=self.host,
                    api_key=self.api_key,
                    port=self.port,
                    https=True
                )
            else:
                self.client = QdrantClient(host=self.host, port=self.port)

            # Test connection
            self.client.get_collections()
            self.connected = True
            logger.info("Successfully connected to Qdrant")
        except Exception as e:
            logger.warning(f"Could not connect to Qdrant: {e}. Running in mock mode.")
            self.client = None
            self.connected = False

        # Create collection if connected and it doesn't exist
        if self.connected:
            self._create_collection()

    def _create_collection(self):
        """Create Qdrant collection for storing document embeddings"""
        try:
            # Check if collection exists
            collections = self.client.get_collections()
            collection_names = [col.name for col in collections.collections]

            if self.collection_name not in collection_names:
                # Create a new collection with appropriate vector size for embedding model
                # Using 768 dimensions, which is common for many embedding models
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE)
                )
                logger.info(f"Created Qdrant collection: {self.collection_name}")
            else:
                logger.info(f"Qdrant collection {self.collection_name} already exists")
        except Exception as e:
            logger.error(f"Error creating Qdrant collection: {e}")

    async def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for the given texts using Gemini service
        """
        import numpy as np
        try:
            from gemini_service import gemini_service

            # Check if we can use Gemini for embeddings
            if gemini_service.api_key and gemini_service.model:
                try:
                    # Try to use Gemini's embedding capabilities if available
                    return await gemini_service.generate_embeddings(texts)
                except Exception as e:
                    logger.warning(f"Gemini embedding failed: {e}. Using fallback embeddings.")
        except ImportError:
            logger.warning("Gemini service not available. Using fallback embeddings.")

        # Fallback to placeholder embeddings
        return await self._generate_placeholder_embeddings(texts)

    async def _generate_placeholder_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate placeholder embeddings when API is unavailable
        """
        import numpy as np
        embeddings = []

        for text in texts:
            # Create a simple hash-based embedding to ensure some similarity for similar texts
            # This is just for demonstration - real embeddings would be generated by the model
            text_hash = hash(text) % (2**32)
            embedding = []

            # Generate a 768-dim vector based on the text content
            for i in range(768):
                val = (text_hash + i * 1337) % 10000
                val = (val - 5000) / 5000.0  # Normalize between -1 and 1
                embedding.append(val)

            # Normalize the embedding vector to unit length (standard for embeddings)
            magnitude = sum(x**2 for x in embedding) ** 0.5
            if magnitude > 0:
                embedding = [x / magnitude for x in embedding]

            embeddings.append(embedding)

        logger.info(f"Generated fallback embeddings for {len(texts)} text(s)")
        return embeddings

    async def store_embeddings(self, texts: List[str], doc_ids: List[str], metadata: List[Dict[str, Any]]) -> List[str]:
        """
        Store embeddings in Qdrant and return the IDs
        """
        if not self.connected:
            logger.warning("Qdrant not connected. Skipping embedding storage.")
            return [str(uuid.uuid4()) for _ in texts]  # Return mock IDs

        try:
            # Generate embeddings for the texts
            embeddings = await self.generate_embeddings(texts)

            # Prepare points for insertion
            points = []
            vector_ids = []

            for i, (text, doc_id, meta) in enumerate(zip(texts, doc_ids, metadata)):
                vector_id = str(uuid.uuid4())
                vector_ids.append(vector_id)

                points.append(
                    PointStruct(
                        id=vector_id,
                        vector=embeddings[i],
                        payload={
                            "text": text,
                            "doc_id": doc_id,
                            **meta
                        }
                    )
                )

            # Store the vectors in Qdrant
            self.client.upsert(
                collection_name=self.collection_name,
                points=points
            )

            logger.info(f"Stored {len(points)} embeddings in Qdrant")
            return vector_ids

        except Exception as e:
            logger.error(f"Error storing embeddings in Qdrant: {e}")
            # Return mock IDs on failure
            return [str(uuid.uuid4()) for _ in texts]

    async def search_similar(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        Search for similar documents to the query
        """
        if not self.connected:
            logger.warning("Qdrant not connected. Returning empty search results.")
            return []  # Return empty results when not connected

        try:
            # Generate embedding for the query
            query_embedding = (await self.generate_embeddings([query]))[0]

            # Search in Qdrant
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=limit
            )

            # Format results
            results = []
            for hit in search_results:
                results.append({
                    "text": hit.payload.get("text", ""),
                    "doc_id": hit.payload.get("doc_id", ""),
                    "score": hit.score,
                    "metadata": {k: v for k, v in hit.payload.items()
                                if k not in ["text", "doc_id"]}
                })

            logger.info(f"Found {len(results)} similar documents for query")
            return results

        except Exception as e:
            logger.error(f"Error searching in Qdrant: {e}")
            return []  # Return empty results on failure

# Singleton instance
qdrant_service = QdrantService()